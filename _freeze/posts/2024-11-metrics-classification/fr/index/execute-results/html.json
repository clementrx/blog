{
  "hash": "18ee0d71dcf39b60a91592093e66ea49",
  "result": {
    "markdown": "---\ntitle: \"Métriques pour la classification\"\nsubtitle: \"Comparer et comprendre les métriques de classification en Python et R\"\ndraft: FALSE\nauthor: [\"Clément Rieux\"]\ncategories: [\"python\", \"classification\", \"R\", \"metriques\"]\ndate: \"2024-11-30\"\nimage: \"../classify.png\"\ntoc: true\nengine: knitr\n---\n\n\n## Exemple de données\n\nNous cherchons à identifier les résultats d'un modèle qui cherche à différencier des prédictions de chiens et de chats. Pour évaluer leur performance, des métriques adaptées permettent de mesurer à quel point un modèle est précis, fiable et équilibré dans ses prédictions.\n\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\n\nn = 100\ndata = pd.DataFrame({\n    'true': ['chien'] * 50 + ['chat'] * 50,\n    'probs': np.concatenate([\n        np.random.uniform(0.5, 1, 50),  \n        np.random.uniform(0, 0.5, 50)   \n    ])\n})\n\ndata['pred'] = data['probs'].apply(lambda x: 'chien' if x > 0.5 else 'chat')\n\nindices_erreurs = np.random.choice(n, 15, replace=False)\ndata.loc[indices_erreurs, 'probs'] = 1 - data.loc[indices_erreurs, 'probs']\ndata.loc[indices_erreurs, 'pred'] = data.loc[indices_erreurs, 'pred'].map({'chien': 'chat', 'chat': 'chien'})\n```\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nn <- 100\ndata <- data.frame(\n  true = factor(c(rep(\"chien\", 50), rep(\"chat\", 50))),   \n  probs = c(runif(50, 0.5, 1), runif(50, 0, 0.5))        \n)\n\ndata$pred <- factor(ifelse(data$probs > 0.5, \"chien\", \"chat\"))\n\nindices_erreurs <- sample(1:n, 15)   \ndata$probs[indices_erreurs] <- 1 - data$probs[indices_erreurs]  \ndata$pred[indices_erreurs] <- ifelse(data$pred[indices_erreurs] == \"chien\", \"chat\", \"chien\") \n```\n:::\n\n:::\n\n## Metriques\n\n### Matrice de confusion\n\nLa matrice de confusion est une représentation en tableau des prédictions correctes et incorrectes :\n\n$$\n\\begin{bmatrix}\nTP & FP \\\\\nFN & TN\n\\end{bmatrix}\n$$\n\n- **TP (True Positives)** : Nombre de prédictions correctes pour la classe positive.\n- **FP (False Positives)** : Nombre de prédictions incorrectes pour la classe positive.\n- **FN (False Negatives)** : Nombre de prédictions incorrectes pour la classe négative.\n- **TN (True Negatives)** : Nombre de prédictions correctes pour la classe négative.\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nconf_matrix = confusion_matrix(data['true'], data['pred'])\nprint(\"Matrice de confusion Python:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrice de confusion Python:\n```\n:::\n\n```{.python .cell-code}\nprint(conf_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[42  8]\n [ 7 43]]\n```\n:::\n\n```{.python .cell-code}\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, \n            annot=True, \n            fmt='d',\n            xticklabels=['chat', 'chien'],\n            yticklabels=['chat', 'chien'])\nplt.title('Matrice de confusion (Python)')\nplt.xlabel('Valeur réelle')\nplt.ylabel('Prédiction')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=576}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nprint(\"Matrice de confusion R:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Matrice de confusion R:\"\n```\n:::\n\n```{.r .cell-code}\ntable(Prédit = data$pred, Réel = data$true)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Réel\nPrédit chat chien\n  chat    44     9\n  chien    6    41\n```\n:::\n\n```{.r .cell-code}\nconf_matrix_r <- as.data.frame(table(data$pred, data$true))\nnames(conf_matrix_r) <- c(\"Prediction\", \"Real\", \"Count\")\n\nggplot(conf_matrix_r, \n       aes(x = Real, y = Prediction, fill = Count)) +\n  geom_tile() +\n  geom_text(aes(label = Count)) +\n  scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n  theme_minimal() +\n  labs(title = \"Matrice de confusion (R)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n:::\n\n:::\n\n### Accuracy\n\nL'**Accuracy** mesure la proportion de prédictions correctes parmi l'ensemble des prédictions.\n\n$$\n\\text{Accuracy} = \\frac{\\text{Vrai Positifs} + \\text{Vrai Négatifs}}{\\text{Total}}\n$$\n\n- **Avantages** : Facile à comprendre et à interpréter. Représente bien la performance globale d'un modèle lorsque les classes sont équilibrées.\n\n- **Inconvénients** : Ne prend pas en compte le déséquilibre des classes. Un modèle peut avoir une haute accuracy même s'il classe mal les classes minoritaires.\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import  accuracy_score\nacc_sklearn = accuracy_score(data['true'], data['pred'])\nprint(f\"Accuracy (sklearn): {acc_sklearn:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy (sklearn): 0.850\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacc_simple <- mean(data$pred == data$true)\nprint(paste(\"Accuracy (simple):\", round(acc_simple, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy (simple): 0.85\"\n```\n:::\n:::\n\n:::\n\n### Précision\n\nLa précision est la proportion des prédictions positives correctes parmi toutes les prédictions positives.\n\n- **Avantages** : Utile lorsque l'on veut minimiser les faux positifs. Par exemple, dans des scénarios où une fausse alerte coûte cher (par exemple, détection de fraude).\n\n- **Inconvénients** : Ne prend pas en compte les faux négatifs, ce qui peut être problématique dans certains cas, par exemple, lorsque l'on souhaite éviter les faux négatifs.\n\n$$\n\\text{Précision} = \\frac{\\text{Vrai Positifs}}{\\text{Vrai Positifs} + \\text{Faux Positifs}}\n$$\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import  precision_score\n\nprecision_per_class = precision_score(\n    data['true'], \n    data['pred'], \n    average=None,\n    labels=['chat', 'chien']\n)\nprint(\"Précision par classe (sklearn):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrécision par classe (sklearn):\n```\n:::\n\n```{.python .cell-code}\nprint(f\"Chat: {precision_per_class[0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChat: 0.857\n```\n:::\n\n```{.python .cell-code}\nprint(f\"Chien: {precision_per_class[1]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChien: 0.843\n```\n:::\n\n```{.python .cell-code}\n# Précision moyenne\nprecision_avg = precision_score(\n    data['true'], \n    data['pred'], \n    average='macro'\n)\nprint(f\"Précision moyenne: {precision_avg:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrécision moyenne: 0.850\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_matrix <- table(Prédit = data$pred, Réel = data$true)\nprecision_chat <- conf_matrix[\"chat\",\"chat\"] / sum(conf_matrix[,\"chat\"])\nprecision_chien <- conf_matrix[\"chien\",\"chien\"] / sum(conf_matrix[,\"chien\"])\n\nprint(\"\\nPrécision par classe (manuel):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"\\nPrécision par classe (manuel):\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Chat:\", round(precision_chat, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Chat: 0.88\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Chien:\", round(precision_chien, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Chien: 0.82\"\n```\n:::\n\n```{.r .cell-code}\nprecision_moy <- mean(c(precision_chat, precision_chien))\nprint(paste(\"Précision moyenne:\", round(precision_moy, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Précision moyenne: 0.85\"\n```\n:::\n:::\n\n:::\n\n\n### Recall\n\nLe Recall mesure la proportion des vrais positifs détectés parmi tous les réels positifs. Il est particulièrement important lorsqu'on veut minimiser les faux négatifs.\n\n- **Avantages** : Utile dans des contextes où il est crucial de capturer autant de cas positifs que possible (par exemple, dans les tests médicaux).\n\n- **Inconvénients** : Ignore les faux positifs, ce qui peut mener à une hausse des faux positifs dans certains cas.\n\n$$\n\\text{Recall} = \\frac{\\text{Vrai Positifs}}{\\text{Vrai Positifs} + \\text{Faux Négatifs}}\n$$\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import  recall_score\n\nrecall_per_class = recall_score(\n    data['true'], \n    data['pred'], \n    average=None,\n    labels=['chat', 'chien']\n)\nprint(\"Recall par classe (sklearn):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecall par classe (sklearn):\n```\n:::\n\n```{.python .cell-code}\nprint(f\"Chat: {recall_per_class[0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChat: 0.840\n```\n:::\n\n```{.python .cell-code}\nprint(f\"Chien: {recall_per_class[1]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChien: 0.860\n```\n:::\n\n```{.python .cell-code}\n# Recall moyen\nrecall_avg = recall_score(\n    data['true'], \n    data['pred'], \n    average='macro'\n)\nprint(f\"Recall moyen: {recall_avg:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecall moyen: 0.850\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_matrix <- table(Prédit = data$pred, Réel = data$true)\nrecall_chat <- conf_matrix[\"chat\",\"chat\"] / sum(conf_matrix[\"chat\",])\nrecall_chien <- conf_matrix[\"chien\",\"chien\"] / sum(conf_matrix[\"chien\",])\n\nprint(\"\\nRecall par classe (manuel):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"\\nRecall par classe (manuel):\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Chat:\", round(recall_chat, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Chat: 0.83\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Chien:\", round(recall_chien, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Chien: 0.872\"\n```\n:::\n\n```{.r .cell-code}\n# Recall moyen\nrecall_moy <- mean(c(recall_chat, recall_chien))\nprint(paste(\"Recall moyen:\", round(recall_moy, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Recall moyen: 0.851\"\n```\n:::\n:::\n\n:::\n\n\n### F1-Score\n\nLe F1-Score est la moyenne harmonique entre la précision et le recall, offrant ainsi un compromis entre les deux. C’est une mesure utile quand il faut équilibrer la précision et le recall\n\n- **Avantages** : Prend en compte à la fois les faux positifs et les faux négatifs. Utile pour les problèmes avec des classes déséquilibrées.\n\n- **Inconvénients** : Si l’un des deux (précision ou recall) est faible, l'F1-score sera également faible.\n\n$$\n\\text{F1-Score} = 2 \\times\\frac{\\text{Précision} \\times \\text{Recall}}{\\text{Précision} + \\text{Recall}}\n$$\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import f1_score\n\n# F1-score par classe\nf1_per_class = f1_score(\n    data['true'], \n    data['pred'], \n    average=None, \n    labels=['chat', 'chien']\n)\nprint(\"F1-Score par classe (sklearn):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nF1-Score par classe (sklearn):\n```\n:::\n\n```{.python .cell-code}\nprint(f\"Chat: {f1_per_class[0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChat: 0.848\n```\n:::\n\n```{.python .cell-code}\nprint(f\"Chien: {f1_per_class[1]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChien: 0.851\n```\n:::\n\n```{.python .cell-code}\n# F1-score moyen\nf1_avg = f1_score(\n    data['true'], \n    data['pred'], \n    average='macro'\n)\nprint(f\"F1-Score moyen: {f1_avg:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nF1-Score moyen: 0.850\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecision_chat <- conf_matrix[\"chat\", \"chat\"] / sum(conf_matrix[, \"chat\"])\nrecall_chat <- conf_matrix[\"chat\", \"chat\"] / sum(conf_matrix[\"chat\", ])\nf1_chat <- 2 * (precision_chat * recall_chat) / (precision_chat + recall_chat)\n\nprecision_chien <- conf_matrix[\"chien\", \"chien\"] / sum(conf_matrix[, \"chien\"])\nrecall_chien <- conf_matrix[\"chien\", \"chien\"] / sum(conf_matrix[\"chien\", ])\nf1_chien <- 2 * (precision_chien * recall_chien) / (precision_chien + recall_chien)\n\nprint(\"\\nF1-Score par classe (manuel):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"\\nF1-Score par classe (manuel):\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Chat:\", round(f1_chat, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Chat: 0.854\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Chien:\", round(f1_chien, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Chien: 0.845\"\n```\n:::\n\n```{.r .cell-code}\n# F1-score moyen\nf1_moy <- mean(c(f1_chat, f1_chien))\nprint(paste(\"F1-Score moyen:\", round(f1_moy, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"F1-Score moyen: 0.85\"\n```\n:::\n:::\n\n:::\n\n### ROC-AUC (Area Under the Curve)\n\nLa courbe ROC est utilisée pour visualiser la performance d'un modèle en fonction de ses seuils de classification. L'AUC mesure la capacité du modèle à distinguer entre les classes.\n\n- **Avantages** : Fonctionne bien avec des classes déséquilibrées et permet une comparaison entre modèles avec des seuils différents.\n\n- **Inconvénients** : Peut être difficile à interpréter dans des contextes très spécifiques.\n\n$$\nAUC = \\int_0^1 \\text{TPR}(fpr) \\, dfpr\n$$\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nroc_auc = roc_auc_score(data['true'] == 'chien', data['probs'])\nprint(f\"ROC-AUC: {roc_auc:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nROC-AUC: 0.841\n```\n:::\n\n```{.python .cell-code}\nfpr, tpr, thresholds = roc_curve(data['true'] == 'chien', data['probs'])\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')\nplt.xlabel('Taux de Faux Positifs (FPR)')\nplt.ylabel('Taux de Vrais Positifs (TPR)')\nplt.title('Courbe ROC')\nplt.legend(loc='lower right')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=768}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pROC)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nType 'citation(\"pROC\")' for a citation.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'pROC'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n```\n:::\n\n```{.r .cell-code}\nroc_auc <- roc(data$true == \"chien\", data$probs)$auc\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting levels: control = FALSE, case = TRUE\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting direction: controls < cases\n```\n:::\n\n```{.r .cell-code}\nprint(sprintf(\"ROC-AUC: %.3f\\n\", roc_auc))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ROC-AUC: 0.856\\n\"\n```\n:::\n\n```{.r .cell-code}\nroc_curve <- roc(data$true == \"chien\", data$probs)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting levels: control = FALSE, case = TRUE\nSetting direction: controls < cases\n```\n:::\n\n```{.r .cell-code}\nplot(roc_curve, main = \"Courbe ROC\", col = \"blue\")\nlegend(\"bottomright\", legend = paste(\"AUC =\", round(roc_auc, 2)), col = \"blue\", lty = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n:::\n\n:::\n\n### Log Loss (Logarithmic Loss) \n\nLe Log-Loss mesure la performance d'un modèle en termes de probabilité. Plus il est bas, meilleure est la qualité des probabilités assignées par le modèle.\n\n- **Avantages** : Prend en compte la confiance du modèle dans ses prédictions, ce qui est important pour évaluer les modèles qui prédisent des probabilités.\n\n- **Inconvénients** : Sensible aux prédictions incorrectes avec une forte confiance.\n\n$$\n\\text{Log Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n$$\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import log_loss\n\nlog_loss_value = log_loss(data['true'], np.vstack([1 - data['probs'], data['probs']]).T, labels=['chat', 'chien'])\nprint(f\"Log-Loss: {log_loss_value:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLog-Loss: 0.520\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata$true_binary <- ifelse(data$true == \"chien\", 1, 0)\n\nlog_loss <- -mean(data$true_binary * log(data$probs) + (1 - data$true_binary) * log(1 - data$probs))\n\nprint(paste(\"Log Loss: \", round(log_loss, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Log Loss:  0.4953\"\n```\n:::\n:::\n\n:::\n\n\n### Matthews Correlation Coefficient (MCC) \n\nLe MCC mesure la corrélation entre les prédictions et les résultats réels, en prenant en compte les vrais positifs, faux positifs, vrais négatifs et faux négatifs. C'est une métrique particulièrement utile pour les classes déséquilibrées.\n\n- **Avantages** : Représente un bon compromis global entre toutes les erreurs possibles (faux positifs, faux négatifs, vrais positifs, vrais négatifs).\n\n- **Inconvénients** : Plus difficile à interpréter que d'autres métriques comme l'accuracy ou le F1-score.\n\n$$\nMCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n$$\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import matthews_corrcoef\n\nmcc_value = matthews_corrcoef(data['true'], data['pred'])\nprint(f\"Matthews Correlation Coefficient (MCC): {mcc_value:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatthews Correlation Coefficient (MCC): 0.700\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mltools)\nmcc_value <- mcc(data$pred, data$true)\nprint(sprintf(\"Matthews Correlation Coefficient (MCC): %.3f\\n\", mcc_value))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Matthews Correlation Coefficient (MCC): 0.701\\n\"\n```\n:::\n:::\n\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}