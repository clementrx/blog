{
  "hash": "7684e302c236acaaefa0070f666575ef",
  "result": {
    "markdown": "---\ntitle: \"Métricas para la clasificación\"\nsubtitle: \"Comparar y comprender las métricas de clasificación en Python y R\"\ndraft: FALSE\nauthor: [\"Clément Rieux\"]\ncategories: [\"python\", \"clasificación\", \"R\", \"métricas\"]\ndate: \"2024-11-30\"\nimage: \"../classify.png\"\ntoc: true\nengine: knitr\n---\n\n\n## Ejemplo de datos\n\nBuscamos identificar los resultados de un modelo que trata de diferenciar predicciones de perros y gatos. Para evaluar su rendimiento, se utilizan métricas adaptadas que permiten medir cuán preciso, confiable y equilibrado es el modelo en sus predicciones.\n\n::: panel-tabset\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\n\nn = 100\ndata = pd.DataFrame({\n    'true': ['perro'] * 50 + ['gato'] * 50,\n    'probs': np.concatenate([\n        np.random.uniform(0.5, 1, 50),  \n        np.random.uniform(0, 0.5, 50)   \n    ])\n})\n\ndata['pred'] = data['probs'].apply(lambda x: 'perro' if x > 0.5 else 'gato')\n\nindices_erreurs = np.random.choice(n, 15, replace=False)\ndata.loc[indices_erreurs, 'probs'] = 1 - data.loc[indices_erreurs, 'probs']\ndata.loc[indices_erreurs, 'pred'] = data.loc[indices_erreurs, 'pred'].map({'perro': 'gato', 'gato': 'perro'})\n```\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nn <- 100\ndata <- data.frame(\n  true = factor(c(rep(\"perro\", 50), rep(\"gato\", 50))),   \n  probs = c(runif(50, 0.5, 1), runif(50, 0, 0.5))        \n)\n\ndata$pred <- factor(ifelse(data$probs > 0.5, \"perro\", \"gato\"))\n\nindices_erreurs <- sample(1:n, 15)   \ndata$probs[indices_erreurs] <- 1 - data$probs[indices_erreurs]  \ndata$pred[indices_erreurs] <- ifelse(data$pred[indices_erreurs] == \"perro\", \"gato\", \"perro\") \n```\n:::\n\n:::\n\n## Métricas\n\n### Matriz de confusión\n\nLa matriz de confusión es una representación en tabla de las predicciones correctas e incorrectas:\n\n\n$$\n\\begin{bmatrix}\nTP & FP \\\\\nFN & TN\n\\end{bmatrix}\n$$\n\n-   **TP (True Positives)** : Número de predicciones correctas para la clase positiva.\n-   **FP (False Positives)** : Número de predicciones incorrectas para la clase positiva.\n-   **FN (False Negatives)** : Número de predicciones incorrectas para la clase negativa.\n-   **TN (True Negatives)** : Número de predicciones correctas para la clase negativa.\n\n::: panel-tabset\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nconf_matrix = confusion_matrix(data['true'], data['pred'])\nprint(\"Matrice de confusion Python:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrice de confusion Python:\n```\n:::\n\n```{.python .cell-code}\nprint(conf_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[42  8]\n [ 7 43]]\n```\n:::\n\n```{.python .cell-code}\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, \n            annot=True, \n            fmt='d',\n            xticklabels=['gato', 'perro'],\n            yticklabels=['gato', 'perro'])\nplt.title('Matrice de confusion (Python)')\nplt.xlabel('Valeur réelle')\nplt.ylabel('Prédiction')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=576}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nprint(\"Matrice de confusion R:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Matrice de confusion R:\"\n```\n:::\n\n```{.r .cell-code}\ntable(Prédit = data$pred, Réel = data$true)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Réel\nPrédit gato perro\n  gato    44     9\n  perro    6    41\n```\n:::\n\n```{.r .cell-code}\nconf_matrix_r <- as.data.frame(table(data$pred, data$true))\nnames(conf_matrix_r) <- c(\"Prediction\", \"Real\", \"Count\")\n\nggplot(conf_matrix_r, \n       aes(x = Real, y = Prediction, fill = Count)) +\n  geom_tile() +\n  geom_text(aes(label = Count)) +\n  scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n  theme_minimal() +\n  labs(title = \"Matrice de confusion (R)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n:::\n\n:::\n\n### Exactitud (Accuracy)\n\nLa Exactitud mide la proporción de predicciones correctas entre el total de predicciones.\n\n$$\n\\text{Exactitud} = \\frac{\\text{Verdaderos Positivos} + \\text{Verdaderos Negativos}}{\\text{Total}}\n$$\n\n-   **Ventajas** : Fácil de entender e interpretar. Representa bien el rendimiento general de un modelo cuando las clases están equilibradas.\n\n-   **Desventajas** : No tiene en cuenta el desequilibrio de clases. Un modelo puede tener una alta exactitud incluso si clasifica mal las clases minoritarias.\n\n::: panel-tabset\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import  accuracy_score\nacc_sklearn = accuracy_score(data['true'], data['pred'])\nprint(f\"Accuracy (sklearn): {acc_sklearn:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy (sklearn): 0.850\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacc_simple <- mean(data$pred == data$true)\nprint(paste(\"Accuracy (simple):\", round(acc_simple, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Accuracy (simple): 0.85\"\n```\n:::\n:::\n\n:::\n\n### Precisión\n\nLa precisión es la proporción de predicciones positivas correctas entre todas las predicciones positivas.\n\n-   **Ventajas** : Útil cuando se quiere minimizar los falsos positivos. Por ejemplo, en escenarios donde una falsa alarma es costosa (como en la detección de fraudes).\n\n-   **Desventajas** : No tiene en cuenta los falsos negativos, lo que puede ser problemático en ciertos casos, como cuando se quiere evitar los falsos negativos.\n\n$$\n\\text{Précision} = \\frac{\\text{Verdaderos Positivos}}{\\text{Verdaderos Positivos} + \\text{Falsos Positivos}}\n$$\n\n::: panel-tabset\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import  precision_score\n\nprecision_per_class = precision_score(\n    data['true'], \n    data['pred'], \n    average=None,\n    labels=['gato', 'perro']\n)\nprint(\"recisión por clase (sklearn):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrecisión por clase (sklearn):\n```\n:::\n\n```{.python .cell-code}\nprint(f\"gato: {precision_per_class[0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ngato: 0.857\n```\n:::\n\n```{.python .cell-code}\nprint(f\"perro: {precision_per_class[1]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nperro: 0.843\n```\n:::\n\n```{.python .cell-code}\n# Précision moyenne\nprecision_avg = precision_score(\n    data['true'], \n    data['pred'], \n    average='macro'\n)\nprint(f\"Precisión promedio: {precision_avg:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrecisión promedio: 0.850\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_matrix <- table(Prédit = data$pred, Réel = data$true)\nprecision_gato <- conf_matrix[\"gato\",\"gato\"] / sum(conf_matrix[,\"gato\"])\nprecision_perro <- conf_matrix[\"perro\",\"perro\"] / sum(conf_matrix[,\"perro\"])\n\nprint(\"\\nrecisión por clase (manuel):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"\\nrecisión por clase (manuel):\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"gato:\", round(precision_gato, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"gato: 0.88\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"perro:\", round(precision_perro, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"perro: 0.82\"\n```\n:::\n\n```{.r .cell-code}\nprecision_moy <- mean(c(precision_gato, precision_perro))\nprint(paste(\"Precisión promedio:\", round(precision_moy, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Precisión promedio: 0.85\"\n```\n:::\n:::\n\n:::\n\n### Recall\n\nEl Recall mide la proporción de verdaderos positivos detectados entre todos los reales positivos. Es especialmente importante cuando se quiere minimizar los falsos negativos.\n\n-   **Ventajas** : Útil en contextos donde es crucial capturar tantos casos positivos como sea posible (por ejemplo, en pruebas médicas).\n\n-   **Desventajas** : Ignora los falsos positivos, lo que puede llevar a un aumento de los falsos positivos en ciertos casos.\n\n$$\n\\text{Recall} = \\frac{\\text{Verdaderos Positivos}}{\\text{Verdaderos Positivos} + \\text{Falsos Negativos}}\n$$\n\n::: panel-tabset\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import  recall_score\n\nrecall_per_class = recall_score(\n    data['true'], \n    data['pred'], \n    average=None,\n    labels=['gato', 'perro']\n)\nprint(\"Recall por classe (sklearn):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecall por classe (sklearn):\n```\n:::\n\n```{.python .cell-code}\nprint(f\"gato: {recall_per_class[0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ngato: 0.840\n```\n:::\n\n```{.python .cell-code}\nprint(f\"perro: {recall_per_class[1]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nperro: 0.860\n```\n:::\n\n```{.python .cell-code}\n# Recall moyen\nrecall_avg = recall_score(\n    data['true'], \n    data['pred'], \n    average='macro'\n)\nprint(f\"Recall promedio: {recall_avg:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecall promedio: 0.850\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_matrix <- table(Prédit = data$pred, Réel = data$true)\nrecall_gato <- conf_matrix[\"gato\",\"gato\"] / sum(conf_matrix[\"gato\",])\nrecall_perro <- conf_matrix[\"perro\",\"perro\"] / sum(conf_matrix[\"perro\",])\n\nprint(\"\\nRecall par classe (manuel):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"\\nRecall par classe (manuel):\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"gato:\", round(recall_gato, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"gato: 0.83\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"perro:\", round(recall_perro, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"perro: 0.872\"\n```\n:::\n\n```{.r .cell-code}\n# Recall moyen\nrecall_moy <- mean(c(recall_gato, recall_perro))\nprint(paste(\"Recall promedio:\", round(recall_moy, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Recall promedio: 0.851\"\n```\n:::\n:::\n\n:::\n\n### F1-Score\n\nEl F1-Score es una medida combinada de la precisión y el recall (recuperación). Es la media armónica de ambas, y es útil cuando se desea equilibrar estas dos métricas. El F1-Score es especialmente útil cuando hay un desbalance entre las clases.\n\n-   **Ventajas** : Toma en cuenta tanto los falsos positivos (FP) como los falsos negativos (FN), lo que lo hace útil en problemas con clases desbalanceadas.\n\n-   **Desventajas** : Si la precisión o el recall son bajos, el F1-Score también será bajo.\n\n$$\n\\text{F1-Score} = 2 \\times\\frac{\\text{Précision} \\times \\text{Recall}}{\\text{Précision} + \\text{Recall}}\n$$\n\n::: panel-tabset\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import f1_score\n\n# F1-score par classe\nf1_per_class = f1_score(\n    data['true'], \n    data['pred'], \n    average=None, \n    labels=['gato', 'perro']\n)\nprint(\"F1-Score par classe (sklearn):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nF1-Score par classe (sklearn):\n```\n:::\n\n```{.python .cell-code}\nprint(f\"gato: {f1_per_class[0]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ngato: 0.848\n```\n:::\n\n```{.python .cell-code}\nprint(f\"perro: {f1_per_class[1]:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nperro: 0.851\n```\n:::\n\n```{.python .cell-code}\n# F1-score moyen\nf1_avg = f1_score(\n    data['true'], \n    data['pred'], \n    average='macro'\n)\nprint(f\"F1-Score promedio: {f1_avg:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nF1-Score promedio: 0.850\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecision_gato <- conf_matrix[\"gato\", \"gato\"] / sum(conf_matrix[, \"gato\"])\nrecall_gato <- conf_matrix[\"gato\", \"gato\"] / sum(conf_matrix[\"gato\", ])\nf1_gato <- 2 * (precision_gato * recall_gato) / (precision_gato + recall_gato)\n\nprecision_perro <- conf_matrix[\"perro\", \"perro\"] / sum(conf_matrix[, \"perro\"])\nrecall_perro <- conf_matrix[\"perro\", \"perro\"] / sum(conf_matrix[\"perro\", ])\nf1_perro <- 2 * (precision_perro * recall_perro) / (precision_perro + recall_perro)\n\nprint(\"\\nF1-Score par classe (manuel):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"\\nF1-Score par classe (manuel):\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"gato:\", round(f1_gato, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"gato: 0.854\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"perro:\", round(f1_perro, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"perro: 0.845\"\n```\n:::\n\n```{.r .cell-code}\n# F1-score moyen\nf1_moy <- mean(c(f1_gato, f1_perro))\nprint(paste(\"F1-Score moyen:\", round(f1_moy, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"F1-Score moyen: 0.85\"\n```\n:::\n:::\n\n:::\n\n### ROC-AUC (Area Under the Curve)\n\nLa curva ROC muestra la relación entre la tasa de verdaderos positivos (TPR) y la tasa de falsos positivos (FPR). El AUC (Área Bajo la Curva) mide la capacidad del modelo para diferenciar entre las clases.\n\n-   **Ventajas** : Es eficaz para evaluar modelos con clases desbalanceadas, y permite comparar modelos con diferentes umbrales de clasificación.\n\n-   **Desventajas** : A veces es difícil de interpretar en contextos muy específicos.\n\n$$\nAUC = \\int_0^1 \\text{TPR}(fpr) \\, dfpr\n$$\n\n::: panel-tabset\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nroc_auc = roc_auc_score(data['true'] == 'perro', data['probs'])\nprint(f\"ROC-AUC: {roc_auc:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nROC-AUC: 0.841\n```\n:::\n\n```{.python .cell-code}\nfpr, tpr, thresholds = roc_curve(data['true'] == 'perro', data['probs'])\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', label=f'Curva ROC (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')\nplt.xlabel('Tasa de Falsos Positivos (FPR)')\nplt.ylabel('Tasa de Verdaderos Positivos (TPR)')\nplt.title('Courbe ROC')\nplt.legend(loc='lower right')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=768}\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pROC)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nType 'citation(\"pROC\")' for a citation.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'pROC'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n```\n:::\n\n```{.r .cell-code}\nroc_auc <- roc(data$true == \"perro\", data$probs)$auc\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting levels: control = FALSE, case = TRUE\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting direction: controls < cases\n```\n:::\n\n```{.r .cell-code}\nprint(sprintf(\"ROC-AUC: %.3f\\n\", roc_auc))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ROC-AUC: 0.856\\n\"\n```\n:::\n\n```{.r .cell-code}\nroc_curve <- roc(data$true == \"perro\", data$probs)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting levels: control = FALSE, case = TRUE\nSetting direction: controls < cases\n```\n:::\n\n```{.r .cell-code}\nplot(roc_curve, main = \"Courbe ROC\", col = \"blue\")\nlegend(\"bottomright\", legend = paste(\"AUC =\", round(roc_auc, 2)), col = \"blue\", lty = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n:::\n\n:::\n\n### Log Loss (Logarithmic Loss)\n\nEl Log-Loss mide la calidad de las probabilidades asignadas por un modelo. Un valor de Log-Loss más bajo indica que el modelo asigna probabilidades más precisas.\n\n-   **Ventajas** : Evalúa la confianza del modelo en sus predicciones, lo cual es importante para modelos que predicen probabilidades.\n\n-   **Desventajas** : Es sensible a las predicciones incorrectas con alta confianza.\n\n$$\n\\text{Log Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n$$\n\n::: panel-tabset\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import log_loss\n\nlog_loss_value = log_loss(data['true'], np.vstack([1 - data['probs'], data['probs']]).T, labels=['gato', 'perro'])\nprint(f\"Log-Loss: {log_loss_value:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLog-Loss: 0.520\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata$true_binary <- ifelse(data$true == \"perro\", 1, 0)\n\nlog_loss <- -mean(data$true_binary * log(data$probs) + (1 - data$true_binary) * log(1 - data$probs))\n\nprint(paste(\"Log Loss: \", round(log_loss, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Log Loss:  0.4953\"\n```\n:::\n:::\n\n:::\n\n### Matthews Correlation Coefficient (MCC)\n\nEl MCC mide la correlación entre las predicciones y los resultados reales, considerando los verdaderos positivos (TP), los falsos positivos (FP), los verdaderos negativos (TN) y los falsos negativos (FN). Es especialmente útil cuando las clases están desbalanceadas.\n\n-   **Ventajas** : Proporciona un buen equilibrio entre todos los posibles errores (falsos positivos, falsos negativos, verdaderos positivos y verdaderos negativos).\n\n-   **Desventajas** : Es más difícil de interpretar que métricas como la precisión o el F1-Score.\n\n$$\nMCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n$$\n\n::: panel-tabset\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import matthews_corrcoef\n\nmcc_value = matthews_corrcoef(data['true'], data['pred'])\nprint(f\"Matthews Correlation Coefficient (MCC): {mcc_value:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatthews Correlation Coefficient (MCC): 0.700\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mltools)\nmcc_value <- mcc(data$pred, data$true)\nprint(sprintf(\"Matthews Correlation Coefficient (MCC): %.3f\\n\", mcc_value))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Matthews Correlation Coefficient (MCC): 0.701\\n\"\n```\n:::\n:::\n\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}