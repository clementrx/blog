{
  "hash": "3cfef5c85146b033d38f9aa7ee178c06",
  "result": {
    "markdown": "---\ntitle: \"Web scraping and tv show anaylsis\"\nsubtitle: \"Get and plot imdb data\"\ndraft: FALSE\nauthor: [\"Clément Rieux\"]\ncategories: [\"scraping\", \"plot\", \"httr\", \"Web Scraping\"]\ndate: \"2023-03-20\"\nimage: \"tv.jpg\"\ntoc: true\n---\n\n\n## Introduction\n\nHello !\n\nWeb scraping has revolutionized the way we collect data from the internet, providing us with valuable insights that were once difficult to obtain.\n\nIn recent years, web scraping has become increasingly popular in the entertainment industry, particularly when it comes to gathering data on TV show ratings and reviews.\n\nBy using web scraping tools, it is now possible to collect large amounts of data on a wide range of TV shows and analyze the data to identify patterns and trends. In this article, we will explore how web scraping can be used to collect TV show data, and how we can use this data to gain insights into the popularity and success of different shows.\n\nWe will also discuss how we can visualize the data using various tools and techniques to help us better understand the information we have gathered. Whether you are a TV enthusiast, a data analyst, or simply someone interested in learning more about the power of web scraping, this article will provide you with valuable insights and practical advice.\n\n![](https://media.giphy.com/media/bjgM8gomF0snK/giphy.gif){style=\"display: block; margin: auto;\" width=\"266\"}\n\n## Where can I find the data ?\n\nActually, it's pretty easy to find rating of tv shows. [imdb](https://www.imdb.com/?ref_=nv_home) provides us all the data we need.\n\nFor example, if I'm looking data for [Breaking Bad](https://www.imdb.com/title/tt0903747/episodes?ref_=tt_eps_sm) :\n\n![](imdb_bb.jpg){fig-align=\"center\"}\n\nI can get multiples informations :\n\n-   Episode Name\n\n-   Rating\n\n-   Number of votes\n\n-   Date\n\n-   Sypnosis\n\nIf I'm looking at the url : `https://www.imdb.com/title/tt0903747/episodes?season=1`, we have :\n\n-   `tt0903747`: Alias for the serie\n\n-   `episodes?season=1`: Season number\n\n## How can i get the data ?\n\n### Load library\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest) # manipulate HTML\nlibrary(stringr) # text manipulation\nlibrary(lubridate) # date manipulation\nlibrary(dplyr) # df manipulation\nlibrary(tidyr) # column manipulation\nlibrary(RColorBrewer) # color\nlibrary(ggplot2) # visualization\n```\n:::\n\n\n### Init the variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tv show title\nserie_title = 'Breaking Bad'\n\n# Seasons\ns = 5\n\n# Tv show code\ncode = 'tt0903747'\n\n# setup dataframe\ntable = data.frame()\n```\n:::\n\n\n### Loop to get informations for each season\n\nFor each season (webpage), we will get informations for each episode.\n\n1.  Get the url\n\n2.  Download the .html page (we can also do it without downloading the page, but I prefer to retrieve the page locally\n\n3.  Extract element, for this i use the extension **SelectorGadget**\n\n4.  Append in the dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(i in 1:s){\n  \n  url = paste0(\"https://www.imdb.com/title/\",code,\"/episodes?season=\",i)\n  \n  while(TRUE){\n    dl_file <- try(download.file(as.character(url), destfile = \"temp.html\", quiet=TRUE),\n                   silent=TRUE)\n    if(!is(dl_file, 'try-error')) break\n  }\n  \n  html_page <- read_html(\"temp.html\")\n  \n  saison = i\n  \n  episode <- html_page %>% \n    html_nodes(\"#episodes_content strong a\") %>% \n    html_text() %>% \n    str_replace_all(\"\\t|\\n|\", \"\")\n  \n  note <- html_page %>% \n    html_nodes(\".ipl-rating-star.small .ipl-rating-star__rating\") %>% \n    html_text() %>% \n    str_replace_all(\"\\t|\\n|\", \"\")\n  \n  vote <- html_page %>% \n    html_nodes(\".ipl-rating-star__total-votes\") %>% \n    html_text() %>% \n    str_replace_all(\"\\t|\\n|\", \"\")\n  \n  date <- html_page %>% \n    html_nodes(\".airdate\") %>% \n    html_text() %>% \n    str_replace_all(\"\\t|\\n|\", \"\") %>% \n    dmy() %>% \n    as.character()\n  \n  temp_table <- as.data.frame(cbind(saison, episode, note, vote, date))\n  table <- rbind(table,temp_table) \n  \n  file.remove('temp.html')\n  \n}\n```\n:::\n\n\nPerfect ! Now we have a well table\n\n\n::: {.cell}\n::: {.cell-output-display}\n|saison |episode                       |note |vote     |date       |\n|:------|:-----------------------------|:----|:--------|:----------|\n|1      |Pilot                         |9.0  |(38,353) |2010-10-09 |\n|1      |Cat's in the Bag...           |8.6  |(27,969) |2008-01-27 |\n|1      |...And the Bag's in the River |8.7  |(27,081) |2008-02-10 |\n|1      |Cancer Man                    |8.2  |(26,101) |2008-02-17 |\n|1      |Gray Matter                   |8.3  |(25,635) |2008-02-24 |\n|1      |Crazy Handful of Nothin'      |9.3  |(30,126) |2008-03-02 |\n:::\n:::\n\n\n## What can I do with the data ?\n\nI use the code from [z3tt](https://github.com/z3tt) for his TidyTuesday [plot from The Office](https://github.com/z3tt/TidyTuesday/blob/main/R/2020_12_TheOffice.Rmd), which I find very cool.\n\nFirstly, we need to compute some informations that will be used to make a graph :\n\n-   Episode number\n\n-   Note by season\n\n-   Rating mean\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_avg <-\n  table %>% \n  group_by(saison) %>% \n  mutate(episode = row_number(),\n         saison = as.numeric(saison),\n         note = as.numeric(note),\n         vote = gsub(',','',vote),\n         vote = as.numeric(gsub(\"[^\\\\d]+\", \"\", vote, perl=TRUE))\n  ) %>% \n  ungroup() %>% \n  arrange(saison, episode) %>% \n  mutate(episode_id = row_number()) %>% \n  group_by(saison) %>% \n  mutate(\n    avg = mean(note),\n    episode_mod = episode_id + (9 * saison),\n    mid = mean(episode_mod)\n  ) %>% \n  ungroup() %>% \n  mutate(saison = factor(saison))\n```\n:::\n\n\nSecondly, for each season summarize the rating :\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_lines <-\n  df_avg %>% \n  group_by(saison) %>% \n  summarize(\n    start_x = min(episode_mod) -5,\n    end_x = max(episode_mod) + 5,\n    y = unique(avg)\n  ) %>% \n  pivot_longer(\n    cols = c(start_x, end_x),\n    names_to = \"type\",\n    values_to = \"x\"\n  ) %>% \n  mutate(\n    x_group = if_else(type == \"start_x\", x + .1, x - .1),\n    x_group = if_else(type == \"start_x\" & x == min(x), x_group - .1, x_group),\n    x_group = if_else(type == \"end_x\" & x == max(x), x_group + .1, x_group)\n  )\n```\n:::\n\n\nThen plot :\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb.cols <- s\nmycolors <- colorRampPalette(brewer.pal(8, \"Paired\"))(nb.cols)\n\ndf_avg %>%\n  ggplot(aes(episode_mod, note)) +\n  geom_hline(\n    data = tibble(y = 7:11),\n    aes(yintercept = y),\n    color = \"grey82\",\n    size = .5\n  ) +\n  geom_segment(aes(\n    xend = episode_mod,\n    yend = avg,\n    color = saison,\n    color = after_scale(colorspace::lighten(color, .2))\n  )) +\n  geom_line(data = df_lines,\n            aes(\n              x_group,\n              y,\n              color = saison,\n              color = after_scale(colorspace::darken(color, .2))\n            ),\n            size = 2.5) +\n  geom_point(aes(size = vote,\n                 color = saison)) +\n  geom_label(\n    aes(\n      mid,\n      10.2,\n      label = glue::glue(\" Saison {saison} \"),\n      color = saison,\n      color = after_scale(colorspace::darken(color, .2))\n    ),\n    show.legend = FALSE,\n    fill = NA,\n    label.padding = unit(.2, \"lines\"),\n    label.r = unit(.25, \"lines\"),\n    label.size = .5\n  )  +\n  scale_x_continuous(expand = c(.015, .015)) +\n  scale_y_continuous(\n    expand = c(.03, .03),\n    limits = c(6.5, 10.5),\n    breaks = seq(6.5, 10, by = .5),\n    sec.axis = dup_axis(name = NULL)\n  ) +\n  scale_color_manual(values = mycolors, guide = \"none\") +\n  scale_size_binned(\n    name = \"Votes per episode\",\n    range = c(.3, 6),\n    labels = function(x)\n      format(x, big.mark = \" \", scientific = FALSE)\n  ) +\n  ggtitle(serie_title) +\n  labs(x = \"\", y = \"Rating\",\n       caption = \"Visualization by Clément Rieux\") +\n  guides(\n    size = guide_bins(\n      show.limits = T,\n      direction = \"horizontal\",\n      title.position = \"top\",\n      title.hjust = .5\n    )\n  ) +\n  theme(\n    legend.position = c(.2, .085),\n    legend.key.size = unit(1, 'cm'),\n    legend.key.width = unit(2, \"lines\"),\n    legend.text = element_text(\n      angle = 60,\n      vjust = 1,\n      hjust = 1\n    ),\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank()\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWe can also look at the evolution over time :\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_avg %>% \n  mutate(date = as.Date(date)) %>%  \n  ggplot(aes(x = date, y = note, color = note)) +\n  geom_point() +\n  scale_colour_gradient(low = \"#E84D23\", high = \"#3CDE0B\", na.value = NA) + \n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%b-%Y\") + \n  theme_classic() + \n  theme(axis.text.x = element_text(angle = 70, hjust = 1)) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nNow you are able to retrieve data from [imdb](https://www.imdb.com/?ref_=nv_home) and do analysis yourself !\n\n![](https://media.giphy.com/media/mGK1g88HZRa2FlKGbz/giphy.gif){fig-align=\"center\" width=\"471\"}\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}